{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pawel/miniconda3/envs/diffusers/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from diffusers.utils import export_to_video\n",
    "\n",
    "from src.hooked_model.hooked_model_ltxvideo import HookedDiffusionModel\n",
    "from src.hooked_model.hooks import AblateHook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"Lightricks/LTX-Video\"\n",
    "model_name = \"a-r-r-o-w/LTX-Video-0.9.1-diffusers\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to register ablation hook and use it during the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.37s/it]\n",
      "Loading pipeline components...: 100%|██████████| 5/5 [00:10<00:00,  2.00s/it]\n"
     ]
    }
   ],
   "source": [
    "pipe = HookedDiffusionModel.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "pipe.to(\"cuda\")\n",
    "pipe.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LTXVideoTransformer3DModel(\n",
       "  (proj_in): Linear(in_features=128, out_features=2048, bias=True)\n",
       "  (time_embed): AdaLayerNormSingle(\n",
       "    (emb): PixArtAlphaCombinedTimestepSizeEmbeddings(\n",
       "      (time_proj): Timesteps()\n",
       "      (timestep_embedder): TimestepEmbedding(\n",
       "        (linear_1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "        (act): SiLU()\n",
       "        (linear_2): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (silu): SiLU()\n",
       "    (linear): Linear(in_features=2048, out_features=12288, bias=True)\n",
       "  )\n",
       "  (caption_projection): PixArtAlphaTextProjection(\n",
       "    (linear_1): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "    (act_1): GELU(approximate='tanh')\n",
       "    (linear_2): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "  )\n",
       "  (rope): LTXVideoRotaryPosEmbed()\n",
       "  (transformer_blocks): ModuleList(\n",
       "    (0-27): 28 x LTXVideoTransformerBlock(\n",
       "      (norm1): RMSNorm()\n",
       "      (attn1): Attention(\n",
       "        (norm_q): RMSNorm()\n",
       "        (norm_k): RMSNorm()\n",
       "        (to_q): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (to_k): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (to_v): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm2): RMSNorm()\n",
       "      (attn2): Attention(\n",
       "        (norm_q): RMSNorm()\n",
       "        (norm_k): RMSNorm()\n",
       "        (to_q): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (to_k): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (to_v): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (net): ModuleList(\n",
       "          (0): GELU(\n",
       "            (proj): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          )\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "          (2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm_out): LayerNorm((2048,), eps=1e-06, elementwise_affine=False)\n",
       "  (proj_out): Linear(in_features=2048, out_features=128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer_blocks.0.attn1\n",
      "transformer_blocks.0.attn2\n",
      "transformer_blocks.1.attn1\n",
      "transformer_blocks.1.attn2\n",
      "transformer_blocks.2.attn1\n",
      "transformer_blocks.2.attn2\n",
      "transformer_blocks.3.attn1\n",
      "transformer_blocks.3.attn2\n",
      "transformer_blocks.4.attn1\n",
      "transformer_blocks.4.attn2\n",
      "transformer_blocks.5.attn1\n",
      "transformer_blocks.5.attn2\n",
      "transformer_blocks.6.attn1\n",
      "transformer_blocks.6.attn2\n",
      "transformer_blocks.7.attn1\n",
      "transformer_blocks.7.attn2\n",
      "transformer_blocks.8.attn1\n",
      "transformer_blocks.8.attn2\n",
      "transformer_blocks.9.attn1\n",
      "transformer_blocks.9.attn2\n",
      "transformer_blocks.10.attn1\n",
      "transformer_blocks.10.attn2\n",
      "transformer_blocks.11.attn1\n",
      "transformer_blocks.11.attn2\n",
      "transformer_blocks.12.attn1\n",
      "transformer_blocks.12.attn2\n",
      "transformer_blocks.13.attn1\n",
      "transformer_blocks.13.attn2\n",
      "transformer_blocks.14.attn1\n",
      "transformer_blocks.14.attn2\n",
      "transformer_blocks.15.attn1\n",
      "transformer_blocks.15.attn2\n",
      "transformer_blocks.16.attn1\n",
      "transformer_blocks.16.attn2\n",
      "transformer_blocks.17.attn1\n",
      "transformer_blocks.17.attn2\n",
      "transformer_blocks.18.attn1\n",
      "transformer_blocks.18.attn2\n",
      "transformer_blocks.19.attn1\n",
      "transformer_blocks.19.attn2\n",
      "transformer_blocks.20.attn1\n",
      "transformer_blocks.20.attn2\n",
      "transformer_blocks.21.attn1\n",
      "transformer_blocks.21.attn2\n",
      "transformer_blocks.22.attn1\n",
      "transformer_blocks.22.attn2\n",
      "transformer_blocks.23.attn1\n",
      "transformer_blocks.23.attn2\n",
      "transformer_blocks.24.attn1\n",
      "transformer_blocks.24.attn2\n",
      "transformer_blocks.25.attn1\n",
      "transformer_blocks.25.attn2\n",
      "transformer_blocks.26.attn1\n",
      "transformer_blocks.26.attn2\n",
      "transformer_blocks.27.attn1\n",
      "transformer_blocks.27.attn2\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "hookpoints = []\n",
    "pattern = re.compile(r\".*transformer_blocks\\.(\\d+).attn(\\d+)$\")\n",
    "for n, m in pipe.transformer.named_modules():\n",
    "    match = pattern.match(n)\n",
    "    if match:\n",
    "        hookpoints.append(n)\n",
    "        print(n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"Spiderman is surfing. Darth Vader is also surfing and following Spiderman\"\n",
    "# prompt = \"Darth vader surfing in waves.\" \n",
    "# prompt = \"A cinematic video of a figure resembling Darth Vader, wearing a black cape and helmet, skillfully surfing powerful ocean waves. The dark figure carves through the water on a sleek black surfboard, cape billowing dramatically in the wind. The sky is cloudy with a hint of sunset glow, casting an epic atmosphere. Water splashes intensely as the figure maintains balance with calm precision. The iconic dark armor glistens under the fading sunlight, evoking a sense of power and mystery.\"\n",
    "# prompt = \"A vibrant, hand-painted animation in the style of Van Gogh's Starry Night. A lone traveler walks down a winding cobblestone street in a swirling, dreamlike village. The sky glows with swirling patterns of deep blue, gold, and violet, while warm yellow streetlights flicker like dancing flames. The traveler, dressed in a flowing coat, pauses to gaze at the mesmerizing sky. The entire scene feels alive, with brushstroke-like textures moving dynamically.\"\n",
    "# negative_prompt = \"Photorealism, sharp details, flat colors, smooth shading, modern cityscapes, digital artifacts, unnatural lighting, robotic movement, sterile environments.\"\n",
    "# prompt = \"A futuristic cyberpunk city at night, bathed in glowing neon lights of magenta, cyan, and electric blue. Sleek motorcycles weave through the streets, their riders clad in high-tech armor. Towering skyscrapers are covered in holographic billboards flashing with digital ads. The air shimmers with faint rain, reflecting vibrant lights off the wet pavement. A mysterious figure in a trench coat walks down a narrow alley, their face illuminated by a flickering hologram.\"\n",
    "# negative_prompt = \"Pastel colors, daylight, rural scenery, low contrast, naturalistic textures, cartoonish characters, minimal detail.\"\n",
    "# prompt = \"A surfer rides a powerful wave in a realistic style. Detailed water splashes, natural lighting, and lifelike motion capture the energy of the ocean.\"\n",
    "# negative_prompt = \"Cartoonish elements, exaggerated motion, flat colors, unnatural lighting, digital artifacts, painterly textures.\"\n",
    "# prompt = \"A woman with long brown hair and light skin smiles at another woman with long blonde hair. The woman with brown hair wears a black jacket and has a small, barely noticeable mole on her right cheek. The camera angle is a close-up, focused on the woman with brown hair's face. The lighting is warm and natural, likely from the setting sun, casting a soft glow on the scene. The scene appears to be real-life footage\"\n",
    "# prompt = \"A woman with long brown hair and light skin smiles at another woman with long blonde hair. The woman with brown hair wears a black jacket and has a small, barely noticeable mole on her right cheek. The camera angle is a close-up, focused on the woman with brown hair's face. The lighting is bright and colorful, with exaggerated shading and smooth, bold outlines. The colors are vibrant, with warm yellows and oranges creating a cheerful, playful tone. The scene appears to be animated in a cartoon style.\"\n",
    "# prompt = \"A pixelated woman with long brown hair and light skin smiles at another pixelated woman with long blonde hair. The woman with brown hair wears a blocky black jacket and has a small, barely noticeable pixel marking her right cheek. The camera angle is a close-up, focused on the woman with brown hair's pixelated face. The lighting is simplified, with bright pixels forming a warm glow on her face. The scene appears to be in a retro 8-bit pixel art style.\"\n",
    "# prompt = \"A distorted, glitchy image of a woman with long brown hair and light skin shows her smiling at another woman with long blonde hair. The woman with brown hair wears a black jacket, and a faint digital artifact marks her right cheek like a small mole. The camera angle is a close-up, focused on the woman with brown hair's face. The lighting is chaotic, with flickering bands of color and digital noise disrupting the scene. Glitches ripple across the image, distorting faces and warping movement. The scene appears heavily corrupted in a glitch art style.\"\n",
    "# prompt = \"A woman with long brown hair and light skin smiles slowly at another woman with long blonde hair. The woman with brown hair wears a black jacket and has a small, barely noticeable mole on her right cheek. The camera angle is a close-up, focused on the woman with brown hair’s face. The lighting is warm and natural, with sunlight casting a soft glow. The motion is slowed down, making the smile unfold gradually, emphasizing subtle facial expressions and the shimmer of light in her eyes. The scene appears to be real-life footage captured in slow motion.\"\n",
    "prompt = \"A woman with long brown hair and light skin smiles at another woman with long blonde hair. The woman with brown hair wears a black jacket and has a small, barely noticeable mole on her right cheek. The camera angle is a close-up, focused on the woman with brown hair’s face. The lighting is warm and natural, with flickering pulses of light creating a strobe effect. The smile appears fragmented, jumping from one expression to the next with brief pauses in between. The scene appears to be real-life footage with a strobe lighting effect.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because `max_sequence_length` is set to  128 tokens: ['effect.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:57<00:00,  1.16s/it]\n",
      "It is recommended to use `export_to_video` with `imageio` and `imageio-ffmpeg` as a backend. \n",
      "These libraries are not present in your environment. Attempting to use legacy OpenCV backend to export video. \n",
      "Support for the OpenCV backend will be deprecated in a future Diffusers version\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'samples/ltxvideo/2_woman_talking/strobe.mp4'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video = pipe(\n",
    "    prompt=prompt,\n",
    "    # negative_prompt=negative_prompt,\n",
    "    width=768,\n",
    "    height=512,\n",
    "    num_frames=161,\n",
    "    num_inference_steps=50,\n",
    ").frames[0]\n",
    "export_to_video(video, f\"samples/ltxvideo/2_woman_talking/strobe.mp4\", fps=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:41<00:00,  1.67s/it]\n",
      "It is recommended to use `export_to_video` with `imageio` and `imageio-ffmpeg` as a backend. \n",
      "These libraries are not present in your environment. Attempting to use legacy OpenCV backend to export video. \n",
      "Support for the OpenCV backend will be deprecated in a future Diffusers version\n",
      "100%|██████████| 25/25 [00:38<00:00,  1.54s/it]\n",
      "It is recommended to use `export_to_video` with `imageio` and `imageio-ffmpeg` as a backend. \n",
      "These libraries are not present in your environment. Attempting to use legacy OpenCV backend to export video. \n",
      "Support for the OpenCV backend will be deprecated in a future Diffusers version\n",
      "100%|██████████| 25/25 [00:39<00:00,  1.58s/it]\n",
      "It is recommended to use `export_to_video` with `imageio` and `imageio-ffmpeg` as a backend. \n",
      "These libraries are not present in your environment. Attempting to use legacy OpenCV backend to export video. \n",
      "Support for the OpenCV backend will be deprecated in a future Diffusers version\n",
      "100%|██████████| 25/25 [00:41<00:00,  1.64s/it]\n",
      "It is recommended to use `export_to_video` with `imageio` and `imageio-ffmpeg` as a backend. \n",
      "These libraries are not present in your environment. Attempting to use legacy OpenCV backend to export video. \n",
      "Support for the OpenCV backend will be deprecated in a future Diffusers version\n",
      "100%|██████████| 25/25 [00:41<00:00,  1.66s/it]\n",
      "It is recommended to use `export_to_video` with `imageio` and `imageio-ffmpeg` as a backend. \n",
      "These libraries are not present in your environment. Attempting to use legacy OpenCV backend to export video. \n",
      "Support for the OpenCV backend will be deprecated in a future Diffusers version\n"
     ]
    }
   ],
   "source": [
    "all_images = []\n",
    "\n",
    "for i, hookpoint in enumerate(hookpoints):\n",
    "    video_frames = pipe.run_with_hooks(\n",
    "        prompt, \n",
    "        num_inference_steps=25, \n",
    "        num_frames=200, \n",
    "        position_hook_dict={hookpoint: AblateHook()},\n",
    "        generator=torch.Generator(device=\"cuda\").manual_seed(1),\n",
    "    ).frames[0]\n",
    "\n",
    "    video_path = export_to_video(video_frames, f\"samples/ablation/{hookpoint}.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(all_images, hookpoints, images_per_row=4):\n",
    "    rows = len(all_images)\n",
    "    fig, axes = plt.subplots(\n",
    "        rows, images_per_row, figsize=(images_per_row * 3, rows * 3)\n",
    "    )\n",
    "    fig.subplots_adjust(hspace=0.5, wspace=0.5)  # Adjust space between rows and columns\n",
    "\n",
    "    for i, row_images in enumerate(all_images[:rows]):  # Limit to the first `rows`\n",
    "        for j, image in enumerate(\n",
    "            row_images[:images_per_row]\n",
    "        ):  # Limit to `images_per_row`\n",
    "            ax = axes[i, j] if rows > 1 else axes[j]  # Handle single row case\n",
    "            ax.imshow(image)\n",
    "            ax.axis(\"off\")  # Turn off axes for a cleaner look\n",
    "            if j == 0:\n",
    "                ax.set_title(hookpoints[i])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "display_images(all_images, hookpoints)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
